    1  su - mohd
    2  ssh-keygen -t rsa -P ""
    3  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    4  chmod 600 ~/.ssh/authorized_keys
    5  ssh-copy-id -i ~/.ssh/id_rsa.pub localhost
    6  ssh localhost
    7  hadoop namenode -format
    8  start-dfs.sh
    9  start-yarn.sh
   10  jps
   11  date -s `curl -I 'https://startpage.com/' 2>/dcv/null | grep -i '^date:' | sed 's/^[Dd]ate: //g'`
   12  su - mohd
   13  nano ~/.bashrc
   14  source ~/.bashrc
   15  su - mohd
   16  bash Anaconda3-5.2.0-Linux-x86_64.sh
   17  spark-shell
   18  nano ~/.bashrc
   19  source ~/.bashrc
   20  spark-shell
   21  su - mohd
   22  nano ~/.bashrc
   23  source ~/.bashrc
   24  su - mohd
   25  hadoop fs -mkdir /usr/
   26  hadoop fs -mkdir /usr/hive/
   27  hadoop fs -mkdir /usr/hive/warehouse
   28  hadoop fs -chmod g+w /usr/hive/warehouse
   29  hive
   30  su - mohd
   31  nano ~/.bashrc
   32  source ~/.bashrc
   33  su - mohd
   34  hive
   35  nano input3.txt
   36  cat input3.txt
   37  hadoop fs -put input3.txt /
   38  hive
   39  hdfs dfs -cat /input3.txt
   40  history > hadoopusercommands.txt
